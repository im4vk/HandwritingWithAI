#!/usr/bin/env python3
"""
Comprehensive Visualization System Testing
==========================================

Test all visualization components to verify rendering capabilities:
- BaseVisualizer
- RealTimeVisualizer  
- TrajectoryPlotter
- RobotRenderer
- MetricsDashboard
- Utility functions
"""

import numpy as np
import matplotlib.pyplot as plt
import sys
import os
import traceback
from pathlib import Path
import json
import time

# Add project root to Python path
project_root = Path(__file__).parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

def test_imports():
    """Test importing all visualization components."""
    print("ðŸ” Testing Visualization System Imports...")
    
    try:
        from src.visualization import (
            BaseVisualizer,
            RealTimeVisualizer,
            TrajectoryPlotter,
            RobotRenderer,
            MetricsDashboard,
            setup_matplotlib,
            create_figure,
            save_plot,
            animate_trajectory,
            create_video,
            setup_colors,
            format_plot
        )
        print("âœ… All visualization components imported successfully")
        return True, {
            'BaseVisualizer': BaseVisualizer,
            'RealTimeVisualizer': RealTimeVisualizer,
            'TrajectoryPlotter': TrajectoryPlotter,
            'RobotRenderer': RobotRenderer,
            'MetricsDashboard': MetricsDashboard,
            'setup_matplotlib': setup_matplotlib,
            'create_figure': create_figure,
            'save_plot': save_plot,
            'animate_trajectory': animate_trajectory,
            'create_video': create_video,
            'setup_colors': setup_colors,
            'format_plot': format_plot
        }
    except Exception as e:
        print(f"âŒ Import failed: {e}")
        traceback.print_exc()
        return False, {}

def generate_test_data():
    """Generate sample data for visualization testing."""
    print("ðŸ“Š Generating test data...")
    
    # Generate sample trajectory
    t = np.linspace(0, 2*np.pi, 100)
    trajectory = np.column_stack([
        0.1 * np.cos(t) + 0.3,
        0.1 * np.sin(t) + 0.2,
        np.full_like(t, 0.02)
    ])
    
    # Generate robot configuration
    robot_config = {
        'joint_positions': np.array([0, 0.5, 0, -1.0, 0, 0.5, 0]),
        'end_effector_position': np.array([0.3, 0.2, 0.02]),
        'workspace_bounds': {
            'x': [0.2, 0.8],
            'y': [-0.3, 0.3], 
            'z': [0.0, 0.5]
        }
    }
    
    # Generate performance metrics
    metrics = {
        'smoothness': 0.85,
        'accuracy': 0.92,
        'speed': 0.78,
        'pressure_consistency': 0.88,
        'trajectory_tracking': 0.91,
        'overall_quality': 0.87
    }
    
    # Generate time series data
    time_series = {
        'time': np.linspace(0, 5, 100),
        'positions': trajectory,
        'velocities': np.random.normal(0, 0.01, (100, 3)),
        'forces': np.random.uniform(0.1, 0.8, 100)
    }
    
    print("âœ… Test data generated successfully")
    return trajectory, robot_config, metrics, time_series

def test_utility_functions(components):
    """Test visualization utility functions."""
    print("\nðŸ”§ Testing Visualization Utilities...")
    
    results = {}
    
    try:
        # Test matplotlib setup
        components['setup_matplotlib']()
        print("âœ… setup_matplotlib() - Works")
        results['setup_matplotlib'] = True
    except Exception as e:
        print(f"âŒ setup_matplotlib() - Failed: {e}")
        results['setup_matplotlib'] = False
    
    try:
        # Test figure creation (correct API: width, height, dpi)
        fig, ax = components['create_figure'](width=800, height=600)
        if fig:
            plt.close(fig)
            print("âœ… create_figure() - Works")
            results['create_figure'] = True
        else:
            print("âŒ create_figure() - Returned None")
            results['create_figure'] = False
    except Exception as e:
        print(f"âŒ create_figure() - Failed: {e}")
        results['create_figure'] = False
    
    try:
        # Test color setup
        colors = components['setup_colors']()
        print(f"âœ… setup_colors() - Works, got {len(colors)} colors")
        results['setup_colors'] = True
    except Exception as e:
        print(f"âŒ setup_colors() - Failed: {e}")
        results['setup_colors'] = False
    
    return results

def test_trajectory_plotter(components, trajectory):
    """Test TrajectoryPlotter component."""
    print("\nðŸ“ˆ Testing TrajectoryPlotter...")
    
    try:
        # Create config for trajectory plotter
        config = {
            'figure_size': (10, 8),
            'show_grid': True,
            'show_legend': True,
            'save_plots': False  # Don't save during testing
        }
        
        plotter = components['TrajectoryPlotter'](config)
        
        # Test initialization
        success = plotter.initialize()
        print(f"âœ… TrajectoryPlotter initialization - {'Works' if success else 'Failed'}")
        
        # Test adding trajectory data
        data = {'trajectory': trajectory}
        plotter.update(data)
        print("âœ… Trajectory data update - Works")
        
        # Test rendering (this calls internal plot methods)
        success = plotter.render()
        print(f"âœ… Trajectory rendering - {'Works' if success else 'Failed'}")
        
        # Test individual plot methods indirectly
        # These are internal methods, so we test via the main render functionality
        print("âœ… Plot methods (path, velocity, acceleration) - Works via render()")
        
        return True
        
    except Exception as e:
        print(f"âŒ TrajectoryPlotter failed: {e}")
        traceback.print_exc()
        return False

def test_robot_renderer(components, robot_config):
    """Test RobotRenderer component."""
    print("\nðŸ¤– Testing RobotRenderer...")
    
    try:
        config = {
            'figure_size': (12, 8),
            'show_workspace': True,
            'show_joints': True,
            'animation_speed': 1.0,
            'renderer_type': 'matplotlib'  # Specify renderer type
        }
        
        renderer = components['RobotRenderer'](config)
        
        # Test initialization
        success = renderer.initialize()
        print(f"âœ… RobotRenderer initialization - {'Works' if success else 'Failed'}")
        
        # Test robot data update
        data = {
            'joint_positions': robot_config['joint_positions'],
            'end_effector_position': robot_config['end_effector_position'],
            'workspace_bounds': robot_config['workspace_bounds']
        }
        renderer.update(data)
        print("âœ… Robot data update - Works")
        
        # Test matplotlib rendering
        success = renderer.render_matplotlib()
        print(f"âœ… Matplotlib rendering - {'Works' if success else 'Failed'}")
        
        # Test plotly rendering (if available)
        try:
            success = renderer.render_plotly()
            print(f"âœ… Plotly rendering - {'Works' if success else 'Failed'}")
        except Exception:
            print("âš ï¸  Plotly rendering - Not available (optional)")
        
        return True
        
    except Exception as e:
        print(f"âŒ RobotRenderer failed: {e}")
        traceback.print_exc()
        return False

def test_metrics_dashboard(components, metrics, time_series):
    """Test MetricsDashboard component."""
    print("\nðŸ“Š Testing MetricsDashboard...")
    
    try:
        config = {
            'figure_size': (15, 10),
            'update_interval': 100,
            'max_history': 1000,
            'layout': 'grid'  # Specify layout type
        }
        
        dashboard = components['MetricsDashboard'](config)
        
        # Test initialization
        success = dashboard.initialize()
        print(f"âœ… MetricsDashboard initialization - {'Works' if success else 'Failed'}")
        
        # Test metrics data update
        data = {
            'metrics': metrics,
            'time_series': time_series
        }
        dashboard.update(data)
        print("âœ… Metrics data update - Works")
        
        # Test dashboard layout creation
        dashboard.create_dashboard_layout()
        print("âœ… Dashboard layout creation - Works")
        
        # Test rendering
        success = dashboard.render()
        print(f"âœ… Dashboard rendering - {'Works' if success else 'Failed'}")
        
        return True
        
    except Exception as e:
        print(f"âŒ MetricsDashboard failed: {e}")
        traceback.print_exc()
        return False

def test_real_time_visualizer(components, trajectory, time_series):
    """Test RealTimeVisualizer component."""
    print("\nâš¡ Testing RealTimeVisualizer...")
    
    try:
        config = {
            'figure_size': (12, 8),
            'update_interval': 50,
            'max_trail_length': 100,
            'enable_3d': True,
            'view_mode': '3d'
        }
        
        visualizer = components['RealTimeVisualizer'](config)
        
        # Test initialization
        success = visualizer.initialize()
        print(f"âœ… Real-time visualizer initialization - {'Works' if success else 'Failed'}")
        
        # Test data update (simulate real-time)
        for i in range(0, min(10, len(trajectory)), 2):  # Test with few points
            data = {
                'pen_position': trajectory[i],
                'timestamp': time_series['time'][i],
                'force': time_series['forces'][i],
                'is_contact': True
            }
            visualizer.update(data)
        print("âœ… Real-time data updates - Works")
        
        # Test rendering
        success = visualizer.render()
        print(f"âœ… Real-time rendering - {'Works' if success else 'Failed'}")
        
        # Test view mode switching
        visualizer.set_view_mode('2d')
        print("âœ… View mode switching - Works")
        
        # Test trajectory clearing
        visualizer.clear_trajectory()
        print("âœ… Trajectory clearing - Works")
        
        return True
        
    except Exception as e:
        print(f"âŒ RealTimeVisualizer failed: {e}")
        traceback.print_exc()
        return False

def test_base_visualizer(components):
    """Test BaseVisualizer component (abstract class)."""
    print("\nðŸ—ï¸ Testing BaseVisualizer...")
    
    try:
        # Test that BaseVisualizer is abstract and can't be instantiated
        config = {'figure_size': (8, 6)}
        
        try:
            base_viz = components['BaseVisualizer'](config)
            print("âŒ BaseVisualizer instantiation - Should have failed (abstract class)")
            return False
        except TypeError as e:
            if "abstract" in str(e).lower():
                print("âœ… BaseVisualizer abstract behavior - Works (correctly prevents instantiation)")
            else:
                print(f"âŒ BaseVisualizer unexpected error: {e}")
                return False
        
        # Test that BaseVisualizer can be imported and has correct interface
        base_class = components['BaseVisualizer']
        
        # Check if it has the expected abstract methods
        expected_methods = ['initialize', 'update', 'render', 'close']
        for method in expected_methods:
            if hasattr(base_class, method):
                print(f"âœ… BaseVisualizer has {method} method")
            else:
                print(f"âŒ BaseVisualizer missing {method} method")
                return False
        
        return True
        
    except Exception as e:
        print(f"âŒ BaseVisualizer failed: {e}")
        traceback.print_exc()
        return False

def run_comprehensive_test():
    """Run comprehensive visualization system test."""
    print("ðŸŽ¨ COMPREHENSIVE VISUALIZATION SYSTEM TESTING")
    print("=" * 55)
    
    # Track results
    test_results = {}
    
    # Test imports
    import_success, components = test_imports()
    test_results['imports'] = import_success
    
    if not import_success:
        print("âŒ Cannot proceed without successful imports")
        return test_results
    
    # Generate test data
    trajectory, robot_config, metrics, time_series = generate_test_data()
    
    # Run individual component tests
    test_results['utilities'] = test_utility_functions(components)
    test_results['base_visualizer'] = test_base_visualizer(components)
    test_results['trajectory_plotter'] = test_trajectory_plotter(components, trajectory)
    test_results['robot_renderer'] = test_robot_renderer(components, robot_config)
    test_results['metrics_dashboard'] = test_metrics_dashboard(components, metrics, time_series)
    test_results['real_time_visualizer'] = test_real_time_visualizer(components, trajectory, time_series)
    
    return test_results

def print_summary(test_results):
    """Print comprehensive test summary."""
    print("\n" + "=" * 55)
    print("ðŸ“Š VISUALIZATION SYSTEM TEST SUMMARY")
    print("=" * 55)
    
    total_tests = 0
    passed_tests = 0
    
    for category, result in test_results.items():
        if isinstance(result, dict):
            # Handle utility functions results
            for test_name, success in result.items():
                total_tests += 1
                if success:
                    passed_tests += 1
                    print(f"   âœ… {category}.{test_name}")
                else:
                    print(f"   âŒ {category}.{test_name}")
        else:
            # Handle component results
            total_tests += 1
            if result:
                passed_tests += 1
                print(f"   âœ… {category}")
            else:
                print(f"   âŒ {category}")
    
    success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
    
    print("=" * 55)
    print(f"ðŸ“ˆ Results: {passed_tests}/{total_tests} tests passed ({success_rate:.1f}%)")
    
    if success_rate >= 90:
        status = "ðŸŸ¢ EXCELLENT"
    elif success_rate >= 70:
        status = "ðŸŸ¡ GOOD"
    elif success_rate >= 50:
        status = "ðŸŸ  PARTIAL"
    else:
        status = "ðŸ”´ NEEDS WORK"
    
    print(f"ðŸŽ¯ Status: {status}")
    
    # Save results
    results_file = "results/visualization_test_results.json"
    os.makedirs("results", exist_ok=True)
    with open(results_file, 'w') as f:
        json.dump({
            'test_results': test_results,
            'summary': {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'success_rate': success_rate,
                'status': status,
                'timestamp': time.time()
            }
        }, f, indent=2)
    
    print(f"ðŸ’¾ Detailed results saved to {results_file}")
    
    return success_rate >= 70  # Return True if mostly successful

if __name__ == "__main__":
    # Set non-interactive matplotlib backend
    import matplotlib
    matplotlib.use('Agg')  # Use non-interactive backend for testing
    
    test_results = run_comprehensive_test()
    success = print_summary(test_results)
    
    sys.exit(0 if success else 1)